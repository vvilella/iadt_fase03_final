{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000771bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers>=4.44.0\" \"datasets>=2.20.0\" \"accelerate>=0.33.0\" \\\n",
    "\"peft>=0.12.0\" \"evaluate>=0.4.2\" \"rouge-score>=0.1.2\" \"sacrebleu>=2.4.2\" torch ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a76b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, random, time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "# ✔ dica MPS: diminui chance de OOM ao reciclar memória\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# caminhos\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.jsonl\"\n",
    "VAL_PATH   = f\"{DATA_DIR}/val.jsonl\"\n",
    "\n",
    "# modelo base (seq2seq) — ótimo pra gerar texto\n",
    "MODEL_NAME = \"google/flan-t5-base\"  # se faltar memória, troque depois pra \"google/flan-t5-small\"\n",
    "\n",
    "# comprimentos (pode ajustar depois)\n",
    "MAX_INPUT_LEN  = 128\n",
    "MAX_TARGET_LEN = 224   # pode subir p/ 256 se estiver estável\n",
    "\n",
    "# hiperparâmetros (pensados p/ M3 Pro, com LoRA)\n",
    "LR        = 3e-4\n",
    "EPOCHS    = 3\n",
    "BATCH     = 2          # pequeno p/ caber em MPS\n",
    "GRAD_ACC  = 8          # acumula gradiente p/ simular batch efetivo 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHED     = \"cosine\"\n",
    "SEED         = 42\n",
    "EVAL_STEPS   = 500\n",
    "SAVE_STEPS   = 500\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# conferências rápidas dos arquivos\n",
    "assert os.path.exists(TRAIN_PATH), f\"faltou {TRAIN_PATH} (gere no 01_data_prep.ipynb)\"\n",
    "assert os.path.exists(VAL_PATH),   f\"faltou {VAL_PATH} (gere no 01_data_prep.ipynb)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02b1ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 48500\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# carrega diretamente dos JSONL\n",
    "raw = load_dataset(\"json\", data_files={\"train\": TRAIN_PATH, \"val\": VAL_PATH})\n",
    "print(raw)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c591186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 48500\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "def preprocess(batch):\n",
    "    # inputs\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    # targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LEN,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=raw[\"train\"].column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)  # definiremos o modelo já-já\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f583362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# carrega modelo base\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "# dicas para treino estável em MPS\n",
    "model.config.use_cache = False               # desliga cache em treino\n",
    "model.gradient_checkpointing_enable()        # menor memória, um pouco mais lento\n",
    "\n",
    "# LoRA config (leve e efetivo)\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,                # rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\"]  # projecções chaves/valores nos attn blocks do T5\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()  # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e1b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decodifica\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    preds, labels = postprocess_text(preds, labels)\n",
    "    rouge_res = rouge.compute(predictions=preds, references=labels, use_aggregator=True)\n",
    "    bleu_res  = bleu.compute(predictions=preds, references=[[l] for l in labels])\n",
    "\n",
    "    # foco em rougeL como métrica-chave\n",
    "    out = {\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "        \"bleu\": bleu_res[\"score\"]\n",
    "    }\n",
    "    return out\n",
    "\n",
    "class EarlyStopper(TrainerCallback):\n",
    "    def __init__(self, metric_name=\"rougeL\", patience=2):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.bad = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None: \n",
    "            return\n",
    "        cur = metrics.get(self.metric_name)\n",
    "        if cur is None:\n",
    "            return\n",
    "        if (self.best is None) or (cur > self.best):\n",
    "            self.best = cur\n",
    "            self.bad = 0\n",
    "        else:\n",
    "            self.bad += 1\n",
    "            if self.bad >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46aeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vilella/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='616' max='9096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 616/9096 19:27 < 4:28:48, 0.53 it/s, Epoch 0.20/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>3.893600</td>\n",
       "      <td>3.451133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d92dbf9f-b378-45d8-bde4-702e8947e541)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e4630313-e08e-4c53-9e19-5fd2dfa39509)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0928e0b1-594c-4ea8-b4c0-344b45d606f7)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     27\u001b[39m data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\u001b[32m     29\u001b[39m trainer = Trainer(\n\u001b[32m     30\u001b[39m     model=model,                      \u001b[38;5;66;03m# reaproveita o modelo já carregado\u001b[39;00m\n\u001b[32m     31\u001b[39m     args=args_noeval,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     ),\n\u001b[32m     39\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m train_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === reconfig \"no-eval\" ===\n",
    "import inspect\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "OUTPUT_DIR = \"outputs/t5_lora_mps\"\n",
    "\n",
    "args_noeval = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH,      # seu BATCH atual\n",
    "    gradient_accumulation_steps=GRAD_ACC,   # seu GRAD_ACC atual\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=LR_SCHED,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,              # salva cedo e sempre\n",
    "    save_total_limit=3,\n",
    "    logging_steps=50,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    bf16=False,\n",
    "    # eval desligado (cobre v4 e v5)\n",
    "    **({\"eval_strategy\": \"no\"} if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters \n",
    "       else {\"evaluation_strategy\": \"no\"})\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                      # reaproveita o modelo já carregado\n",
    "    args=args_noeval,\n",
    "    train_dataset=tokenized[\"train\"], # pode reduzir mais tarde se quiser\n",
    "    data_collator=data_collator,\n",
    "    **(\n",
    "        {\"processing_class\": tokenizer}\n",
    "        if \"processing_class\" in inspect.signature(Trainer.__init__).parameters\n",
    "        else {\"tokenizer\": tokenizer}\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd7d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando checkpoint: outputs/t5_lora_mps/checkpoint-600\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import torch, os, json\n",
    "\n",
    "ckpt_path = \"outputs/t5_lora_mps/checkpoint-600\"\n",
    "print(\"Usando checkpoint:\", ckpt_path)\n",
    "\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base.to(device)\n",
    "\n",
    "model_ckpt = PeftModel.from_pretrained(base, ckpt_path)\n",
    "model_ckpt.to(device)\n",
    "\n",
    "trainer.model = model_ckpt  # substitui modelo atual\n",
    "\n",
    "# config de avaliação \"magra\" (pra não estourar memória)\n",
    "if hasattr(trainer.args, \"per_device_eval_batch_size\"): trainer.args.per_device_eval_batch_size = 1\n",
    "if hasattr(trainer.args, \"include_logits_in_eval\"):     trainer.args.include_logits_in_eval = False\n",
    "if hasattr(trainer.args, \"eval_accumulation_steps\"):    trainer.args.eval_accumulation_steps = 1\n",
    "\n",
    "setattr(trainer, \"_gen_kwargs\", {\"max_new_tokens\": MAX_TARGET_LEN, \"num_beams\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6272f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do val sample: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vilella/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas checkpoint-600: {'eval_loss': 3.4426136016845703}\n"
     ]
    }
   ],
   "source": [
    "small_val = tokenized[\"val\"].select(range(min(1000, len(tokenized[\"val\"]))))\n",
    "print(\"Tamanho do val sample:\", len(small_val))\n",
    "\n",
    "eval_metrics_600 = trainer.evaluate(eval_dataset=small_val)\n",
    "print(\"Métricas checkpoint-600:\", eval_metrics_600)\n",
    "\n",
    "# salva snapshot de métricas\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "with open(\"outputs/eval_ckpt600.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_metrics_600, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02d9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação completa: {'eval_loss': 3.4511327743530273}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics_full = trainer.evaluate(eval_dataset=tokenized[\"val\"])\n",
    "print(\"Avaliação completa:\", eval_metrics_full)\n",
    "\n",
    "with open(\"outputs/eval_ckpt600_full.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_metrics_full, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cf456722-8784-4f2d-b369-4dff93818314)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Modelo final salvo em artifacts/t5_lora\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"artifacts/t5_lora\", exist_ok=True)\n",
    "trainer.save_model(\"artifacts/t5_lora\")\n",
    "tokenizer.save_pretrained(\"artifacts/t5_lora\")\n",
    "print(\"Modelo final salvo em artifacts/t5_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb7ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cd9520d2-bd3e-4bb9-aa98-4f870f92a7f1)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparação salva em: outputs/compare_baseline_vs_finetuned.jsonl\n",
      "{\"input\": \"Given a product title, generate its product description.\\nTitle: Islandoffer Color Pearl Nail Art Stone Small Wheel Rhinestones Beads\\nDescription:\", \"baseline_pred\": \"Islandoffer Color Pearl Nail Art Stone Small Wheel Rhinestones Beads\", \"finetuned_pred\": \"Islandoffer Color Pearl Nail Art Stone Small Wheel Rhinestones Beads\", \"ref\": \"Make your nails look elegance and special,Suitable to use on top of nail polish, UV builder gel, acrylic, etc,Also can be used to decorate your home and furniture, cell phone and mp3 cases, glasses, cards, body art, bookmarks.\"} ...\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "baseline_path = \"outputs/baseline_val200.jsonl\"\n",
    "\n",
    "# 0) util de geração\n",
    "def gen_batch(model, texts, tok, device, max_in=128, max_out=256, bs=4):\n",
    "    out = []\n",
    "    for i in range(0, len(texts), bs):\n",
    "        enc = tok(texts[i:i+bs], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_in).to(device)\n",
    "        with torch.no_grad():\n",
    "            ids = model.generate(**enc, max_new_tokens=max_out, num_beams=1)\n",
    "        out += tok.batch_decode(ids, skip_special_tokens=True)\n",
    "    return out\n",
    "\n",
    "# 1) inputs (usa baseline se existir; senão cria 200 do val)\n",
    "if os.path.exists(baseline_path):\n",
    "    rows = [json.loads(l) for l in open(baseline_path, \"r\", encoding=\"utf-8\")]\n",
    "    inputs = [r[\"input\"] for r in rows]\n",
    "    refs   = [r[\"ref\"]   for r in rows]\n",
    "else:\n",
    "    # seleciona 200 primeiros do val (ou aleatórios)\n",
    "    N = min(200, len(tokenized[\"val\"]))\n",
    "    sample = tokenized[\"val\"].select(range(N))\n",
    "    inputs = sample[\"input_text\"]\n",
    "    refs   = sample[\"target_text\"]\n",
    "\n",
    "# 2) baseline = FLAN-T5-base \"cru\"\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "preds_base = gen_batch(base_model, inputs, tokenizer, device, MAX_INPUT_LEN, MAX_TARGET_LEN)\n",
    "\n",
    "# 3) fine-tuned = seu checkpoint carregado como `model_ckpt`\n",
    "preds_ft = gen_batch(model_ckpt, inputs, tokenizer, device, MAX_INPUT_LEN, MAX_TARGET_LEN)\n",
    "\n",
    "# 4) salvar comparação\n",
    "out_path = \"outputs/compare_baseline_vs_finetuned.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for x, pb, pf, r in zip(inputs, preds_base, preds_ft, refs):\n",
    "        f.write(json.dumps({\"input\": x, \"baseline_pred\": pb, \"finetuned_pred\": pf, \"ref\": r}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Comparação salva em:\", out_path)\n",
    "# mostre 1 exemplo\n",
    "print(json.dumps({\"input\": inputs[0], \"baseline_pred\": preds_base[0], \"finetuned_pred\": preds_ft[0], \"ref\": refs[0]}, ensure_ascii=False)[:600], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f23db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline → {'rougeL': np.float64(0.1223), 'bleu': 0.0003}\n",
      "Fine-tuned → {'rougeL': np.float64(0.1266), 'bleu': 1.2264}\n"
     ]
    }
   ],
   "source": [
    "import evaluate, json\n",
    "\n",
    "cmp_path = \"outputs/compare_baseline_vs_finetuned.jsonl\"\n",
    "assert os.path.exists(cmp_path), \"rode o bloco de comparação primeiro\"\n",
    "\n",
    "rows = [json.loads(l) for l in open(cmp_path, \"r\", encoding=\"utf-8\")]\n",
    "preds_ft = [r[\"finetuned_pred\"] for r in rows]\n",
    "refs     = [r[\"ref\"]           for r in rows]\n",
    "preds_bl = [r[\"baseline_pred\"] for r in rows]\n",
    "\n",
    "rouge = evaluate.load(\"rouge\"); bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def metrics_of(preds, refs):\n",
    "    r = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\n",
    "    b = bleu.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    return {\"rougeL\": r[\"rougeL\"], \"bleu\": b[\"score\"]}\n",
    "\n",
    "m_ft = metrics_of(preds_ft, refs)\n",
    "m_bl = metrics_of(preds_bl, refs)\n",
    "\n",
    "print(\"Baseline →\", {k: round(v, 4) for k,v in m_bl.items()})\n",
    "print(\"Fine-tuned →\", {k: round(v, 4) for k,v in m_ft.items()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
