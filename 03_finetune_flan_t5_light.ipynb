{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000771bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers>=4.44.0\" \"datasets>=2.20.0\" \"accelerate>=0.33.0\" \\\n",
    "\"peft>=0.12.0\" \"evaluate>=0.4.2\" \"rouge-score>=0.1.2\" \"sacrebleu>=2.4.2\" torch ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a76b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, random, time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "# ✔ dica MPS: diminui chance de OOM ao reciclar memória\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# caminhos\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.jsonl\"\n",
    "VAL_PATH   = f\"{DATA_DIR}/val.jsonl\"\n",
    "\n",
    "# modelo base (seq2seq) — ótimo pra gerar texto\n",
    "MODEL_NAME = \"google/flan-t5-base\"  # se faltar memória, troque depois pra \"google/flan-t5-small\"\n",
    "\n",
    "# comprimentos (pode ajustar depois)\n",
    "MAX_INPUT_LEN  = 128\n",
    "MAX_TARGET_LEN = 224   # pode subir p/ 256 se estiver estável\n",
    "\n",
    "# hiperparâmetros (pensados p/ M3 Pro, com LoRA)\n",
    "LR        = 3e-4\n",
    "EPOCHS    = 3\n",
    "BATCH     = 2          # pequeno p/ caber em MPS\n",
    "GRAD_ACC  = 8          # acumula gradiente p/ simular batch efetivo 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHED     = \"cosine\"\n",
    "SEED         = 42\n",
    "EVAL_STEPS   = 500\n",
    "SAVE_STEPS   = 500\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# conferências rápidas dos arquivos\n",
    "assert os.path.exists(TRAIN_PATH), f\"faltou {TRAIN_PATH} (gere no 01_data_prep.ipynb)\"\n",
    "assert os.path.exists(VAL_PATH),   f\"faltou {VAL_PATH} (gere no 01_data_prep.ipynb)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02b1ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4763770a056a476a8259382df276b797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab7f4ff3f6f43e0849a67a7d1cb6d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 48500\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# carrega diretamente dos JSONL\n",
    "raw = load_dataset(\"json\", data_files={\"train\": TRAIN_PATH, \"val\": VAL_PATH})\n",
    "print(raw)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c591186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b517ef7d554418c8aec9d1558e8ebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vilella/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2649869d7414463a8f9ca212169e94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 48500\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "def preprocess(batch):\n",
    "    # inputs\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    # targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LEN,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=raw[\"train\"].column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)  # definiremos o modelo já-já\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f583362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# carrega modelo base\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "# dicas para treino estável em MPS\n",
    "model.config.use_cache = False               # desliga cache em treino\n",
    "model.gradient_checkpointing_enable()        # menor memória, um pouco mais lento\n",
    "\n",
    "# LoRA config (leve e efetivo)\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,                # rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\"]  # projecções chaves/valores nos attn blocks do T5\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()  # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e1b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decodifica\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    preds, labels = postprocess_text(preds, labels)\n",
    "    rouge_res = rouge.compute(predictions=preds, references=labels, use_aggregator=True)\n",
    "    bleu_res  = bleu.compute(predictions=preds, references=[[l] for l in labels])\n",
    "\n",
    "    # foco em rougeL como métrica-chave\n",
    "    out = {\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "        \"bleu\": bleu_res[\"score\"]\n",
    "    }\n",
    "    return out\n",
    "\n",
    "class EarlyStopper(TrainerCallback):\n",
    "    def __init__(self, metric_name=\"rougeL\", patience=2):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.bad = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None: \n",
    "            return\n",
    "        cur = metrics.get(self.metric_name)\n",
    "        if cur is None:\n",
    "            return\n",
    "        if (self.best is None) or (cur > self.best):\n",
    "            self.best = cur\n",
    "            self.bad = 0\n",
    "        else:\n",
    "            self.bad += 1\n",
    "            if self.bad >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46aeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/kmbrcc390p9gx2s3pl8ld_fc0000gn/T/ipykernel_99644/1834163386.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x32ed40ad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"outputs/t5_lora_mps\"\n",
    "\n",
    "# base comum (sem args problemáticos)\n",
    "common_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=LR_SCHED,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    fp16=(device == \"cuda\"),   # no MPS, manter False\n",
    "    bf16=False,\n",
    ")\n",
    "\n",
    "# 1) tentar usar Seq2SeqTrainingArguments (tem predict_with_generate em algumas versões)\n",
    "try:\n",
    "    from transformers import Seq2SeqTrainingArguments as _TA\n",
    "except Exception:\n",
    "    _TA = TrainingArguments\n",
    "\n",
    "# 2) montar kwargs de forma dinâmica, conforme assinatura da classe\n",
    "sig = inspect.signature(_TA.__init__)\n",
    "allowed = set(sig.parameters.keys())\n",
    "\n",
    "dyn_kwargs = dict(common_kwargs)\n",
    "\n",
    "# evaluation strategy mudou de nome em algumas versões\n",
    "if \"eval_strategy\" in allowed:\n",
    "    dyn_kwargs[\"eval_strategy\"] = \"steps\"\n",
    "elif \"evaluation_strategy\" in allowed:\n",
    "    dyn_kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "# se nenhum dos dois existir, seguimos sem estratégia explícita (fazemos eval manual se precisar)\n",
    "\n",
    "# predict_with_generate existe em Seq2SeqTrainingArguments (algumas versões)\n",
    "if \"predict_with_generate\" in allowed:\n",
    "    dyn_kwargs[\"predict_with_generate\"] = True\n",
    "\n",
    "# geração: em v5, passe via gen_kwargs no Trainer; guardamos aqui para injetar depois\n",
    "gen_kwargs = {\"max_new_tokens\": MAX_TARGET_LEN}\n",
    "\n",
    "args = _TA(**dyn_kwargs)\n",
    "\n",
    "# reconstroi o data_collator agora com o modelo definido\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStopper(metric_name=\"rougeL\", patience=2)]\n",
    ")\n",
    "\n",
    "# hack compatível: injeta gen_kwargs para avaliação/predição\n",
    "setattr(trainer, \"_gen_kwargs\", gen_kwargs)\n",
    "\n",
    "trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96047ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vilella/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='9096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/9096 15:51 < 4:33:03, 0.52 it/s, Epoch 0.16/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [167/750 17:45 < 1:02:23, 0.16 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 9.01 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m train_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:2754\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2752\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2754\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2765\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:3227\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3225\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3227\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3228\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:3176\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3176\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3177\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3179\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:4469\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4466\u001b[39m start_time = time.time()\n\u001b[32m   4468\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4469\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4470\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4472\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4473\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4479\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer.py:4692\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4690\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.gather_function(logits)\n\u001b[32m   4691\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.batch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description == \u001b[33m\"\u001b[39m\u001b[33mPrediction\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4692\u001b[39m         \u001b[43mall_preds\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4693\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4694\u001b[39m     labels = \u001b[38;5;28mself\u001b[39m.gather_function(labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:315\u001b[39m, in \u001b[36mEvalLoopContainer.add\u001b[39m\u001b[34m(self, tensors)\u001b[39m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors.append(tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:129\u001b[39m, in \u001b[36mnested_concat\u001b[39m\u001b[34m(tensors, new_tensors, padding_index)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[32m    126\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch.Tensor):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:129\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[32m    126\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch.Tensor):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:131\u001b[39m, in \u001b[36mnested_concat\u001b[39m\u001b[34m(tensors, new_tensors, padding_index)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[32m    134\u001b[39m         {k: nested_concat(t, new_tensors[k], padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors.items()}\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:89\u001b[39m, in \u001b[36mtorch_pad_and_concatenate\u001b[39m\u001b[34m(tensor1, tensor2, padding_index)\u001b[39m\n\u001b[32m     86\u001b[39m tensor2 = atleast_1d(tensor2)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1.shape) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1.shape[\u001b[32m1\u001b[39m] == tensor2.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[32m     92\u001b[39m new_shape = (tensor1.shape[\u001b[32m0\u001b[39m] + tensor2.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1.shape[\u001b[32m1\u001b[39m], tensor2.shape[\u001b[32m1\u001b[39m])) + tensor1.shape[\u001b[32m2\u001b[39m:]\n",
      "\u001b[31mRuntimeError\u001b[39m: Invalid buffer size: 9.01 GiB"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e34db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints encontrados: []\n",
      "Último checkpoint: None\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json, pprint\n",
    "\n",
    "OUTDIR = \"outputs/t5_lora_mps\"\n",
    "\n",
    "ckpts = sorted(\n",
    "    glob.glob(f\"{OUTDIR}/checkpoint-*\"),\n",
    "    key=lambda p: int(p.split(\"-\")[-1])\n",
    ")\n",
    "print(\"Checkpoints encontrados:\", [os.path.basename(p) for p in ckpts][-5:])\n",
    "last_ckpt = ckpts[-1] if ckpts else None\n",
    "print(\"Último checkpoint:\", last_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbab62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_state.json não encontrado\n"
     ]
    }
   ],
   "source": [
    "STATE = f\"{OUTDIR}/trainer_state.json\"\n",
    "if os.path.exists(STATE):\n",
    "    with open(STATE, \"r\", encoding=\"utf-8\") as f:\n",
    "        st = json.load(f)\n",
    "    print(\"global_step:\", st.get(\"global_step\"))\n",
    "    print(\"epoch:\", st.get(\"epoch\"))\n",
    "    print(\"best_metric (rougeL):\", st.get(\"best_metric\"))\n",
    "    print(\"best_model_checkpoint:\", st.get(\"best_model_checkpoint\"))\n",
    "    print(\"\\nÚltimos logs:\")\n",
    "    for e in st.get(\"log_history\", [])[-5:]:\n",
    "        pprint.pprint(e)\n",
    "else:\n",
    "    print(\"trainer_state.json não encontrado\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
