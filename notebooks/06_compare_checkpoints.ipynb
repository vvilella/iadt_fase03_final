{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f3f2eb",
   "metadata": {},
   "source": [
    "# 05_compare_checkpoints.ipynb\n",
    "\n",
    "Comparação objetiva entre checkpoints do fine-tuning (ex.: 6064 vs 9000).\n",
    "\n",
    "Este notebook NÃO treina. Ele apenas:\n",
    "1) Localiza/copiar checkpoints.\n",
    "2) Avalia em val(1k).\n",
    "3) Compara em 200 amostras (baseline já existente).\n",
    "4) Escolhe o melhor e atualiza `artifacts/t5_lora_best/`.\n",
    "\n",
    "> Ajuste o caminho do backup do checkpoint-9000 na célula 1 se necessário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ccc9d",
   "metadata": {},
   "source": [
    "## Utilitários de carregamento/avaliação (val 1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c80ca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Val utilizado: 1000\n"
     ]
    }
   ],
   "source": [
    "import torch, json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# dataset cru (strings)\n",
    "raw_val = load_dataset(\"json\", data_files={\"val\":\"../data/val.jsonl\"})[\"val\"]\n",
    "N = min(1000, len(raw_val))\n",
    "texts = raw_val.select(range(N))[\"input_text\"]\n",
    "refs  = raw_val.select(range(N))[\"target_text\"]\n",
    "print(\"Val utilizado:\", N)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_peft_from_ckpt(base_model_name: str, ckpt_path: str):\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "    mdl = PeftModel.from_pretrained(base, ckpt_path).to(device)\n",
    "    mdl.eval()\n",
    "    return mdl\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_checkpoint(model, tokenizer, texts, refs, max_in=128, max_out=224, beams=1):\n",
    "    preds=[]\n",
    "    for x in texts:\n",
    "        enc = tokenizer(x, return_tensors=\"pt\", truncation=True, max_length=max_in).to(device)\n",
    "        out = model.generate(**enc, max_new_tokens=max_out, num_beams=beams)\n",
    "        preds.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        if device==\"mps\": torch.mps.empty_cache()\n",
    "    r = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\n",
    "    b = bleu.compute(predictions=preds, references=[[y] for y in refs])\n",
    "    return {\"rougeL\": float(r[\"rougeL\"]), \"bleu\": float(b[\"score\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ee781",
   "metadata": {},
   "source": [
    "## Avaliar 6064 e 9000 em val(1k) e salvar snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a304b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6064] ROUGE-L=0.1008 | BLEU=0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f2fd3a1a-9283-449d-80fe-e964b1b93c57)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f2fd3a1a-9283-449d-80fe-e964b1b93c57)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000] ROUGE-L=0.1259 | BLEU=1.19\n",
      "Snapshot salvo em outputs/eval_ckpt_6064_vs_9000.json\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "CKPT_9000_DEST = \"../outputs/t5_lora_mps/checkpoint-9000\"  # mesmo da célula 1\n",
    "CKPT_6064 = \"../outputs/t5_lora_mps/checkpoint-6064\"\n",
    "\n",
    "ckpts_to_eval = {}\n",
    "if os.path.isdir(CKPT_6064):\n",
    "    ckpts_to_eval[\"6064\"] = CKPT_6064\n",
    "if os.path.isdir(CKPT_9000_DEST):\n",
    "    ckpts_to_eval[\"9000\"] = CKPT_9000_DEST\n",
    "\n",
    "assert ckpts_to_eval, \"Nenhum checkpoint encontrado para avaliar.\"\n",
    "\n",
    "results = {}\n",
    "for tag, path in ckpts_to_eval.items():\n",
    "    model = load_peft_from_ckpt(MODEL_NAME, path)\n",
    "    metrics = eval_checkpoint(model, tokenizer, texts, refs, max_in=128, max_out=224, beams=1)\n",
    "    results[tag] = metrics\n",
    "    print(f\"[{tag}] ROUGE-L={metrics['rougeL']:.4f} | BLEU={metrics['bleu']:.2f}\")\n",
    "\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "with open(\"../outputs/eval_ckpt_6064_vs_9000.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Snapshot salvo em outputs/eval_ckpt_6064_vs_9000.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b4cef",
   "metadata": {},
   "source": [
    "## Comparação “200 amostras” (gera arquivos por checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d11cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8c6e6250-2f72-4fd3-9c12-c555d77783dc)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8c6e6250-2f72-4fd3-9c12-c555d77783dc)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparação salva: ../outputs/compare_finetuned_6064_vs_baseline.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 60e55549-88ae-4553-a813-f7df5ad04466)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 60e55549-88ae-4553-a813-f7df5ad04466)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparação salva: ../outputs/compare_finetuned_9000_vs_baseline.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, torch, os\n",
    "\n",
    "# carrega baseline 200 já existente\n",
    "rows = [json.loads(l) for l in open(\"../outputs/baseline_val200.jsonl\",\"r\",encoding=\"utf-8\")]\n",
    "inputs = [r[\"input\"] for r in rows]\n",
    "refs200 = [r[\"ref\"] for r in rows]\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_preds(model, tokenizer, texts, bs=2, max_in=128, max_out=224, beams=1):\n",
    "    out=[]\n",
    "    for i in range(0,len(texts),bs):\n",
    "        enc = tokenizer(texts[i:i+bs], return_tensors=\"pt\", padding=True,\n",
    "                        truncation=True, max_length=max_in).to(device)\n",
    "        ids = model.generate(**enc, max_new_tokens=max_out, num_beams=beams)\n",
    "        out += tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "        if device==\"mps\": torch.mps.empty_cache()\n",
    "    return out\n",
    "\n",
    "def compare_and_save(tag, ckpt_path):\n",
    "    model = load_peft_from_ckpt(MODEL_NAME, ckpt_path)\n",
    "    preds = gen_preds(model, tokenizer, inputs, bs=2, max_in=128, max_out=224, beams=1)\n",
    "    out_path = f\"../outputs/compare_finetuned_{tag}_vs_baseline.jsonl\"\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for x, pf, y in zip(inputs, preds, refs200):\n",
    "            f.write(json.dumps({\"input\": x, \"finetuned_pred\": pf, \"ref\": y}, ensure_ascii=False)+\"\\n\")\n",
    "    print(f\"Comparação salva: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "if os.path.isdir(CKPT_6064):\n",
    "    compare_and_save(\"6064\", CKPT_6064)\n",
    "if os.path.isdir(CKPT_9000_DEST):\n",
    "    compare_and_save(\"9000\", CKPT_9000_DEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035ab15",
   "metadata": {},
   "source": [
    "## Métricas nas 200 amostras (ambos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6aa1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 amostras — 6064 → {'rougeL': 0.1117, 'bleu': 0.9039}\n",
      "200 amostras — 9000 → {'rougeL': 0.1266, 'bleu': 1.2264}\n",
      "Métricas salvas em outputs/metrics_200_ckpt_6064_vs_9000.json\n"
     ]
    }
   ],
   "source": [
    "import evaluate, json, os\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def load_preds(file_path):\n",
    "    rows = [json.loads(l) for l in open(file_path,\"r\",encoding=\"utf-8\")]\n",
    "    return [r[\"finetuned_pred\"] for r in rows], [r[\"ref\"] for r in rows]\n",
    "\n",
    "m = {}\n",
    "\n",
    "p6064 = \"../outputs/compare_finetuned_6064_vs_baseline.jsonl\"\n",
    "p9000 = \"../outputs/compare_finetuned_9000_vs_baseline.jsonl\"\n",
    "\n",
    "def metric(preds, refs):\n",
    "    r = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\n",
    "    b = bleu.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    return {\"rougeL\": float(r[\"rougeL\"]), \"bleu\": float(b[\"score\"])}\n",
    "\n",
    "if os.path.exists(p6064):\n",
    "    preds, refs = load_preds(p6064)\n",
    "    m[\"6064\"] = {k: round(v,4) for k,v in metric(preds, refs).items()}\n",
    "    print(\"200 amostras — 6064 →\", m[\"6064\"])\n",
    "\n",
    "if os.path.exists(p9000):\n",
    "    preds, refs = load_preds(p9000)\n",
    "    m[\"9000\"] = {k: round(v,4) for k,v in metric(preds, refs).items()}\n",
    "    print(\"200 amostras — 9000 →\", m.get(\"9000\"))\n",
    "\n",
    "with open(\"../outputs/metrics_200_ckpt_6064_vs_9000.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(m, f, indent=2)\n",
    "print(\"Métricas salvas em outputs/metrics_200_ckpt_6064_vs_9000.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f82fc2",
   "metadata": {},
   "source": [
    "## Selecionar o melhor checkpoint e atualizar `artifacts/t5_lora_best/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01363a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas carregadas: {'6064': {'rougeL': 0.1117, 'bleu': 0.9039}, '9000': {'rougeL': 0.1266, 'bleu': 1.2264}}\n",
      "Melhor: 9000 → ../outputs/t5_lora_mps/checkpoint-9000\n",
      "Modelo final atualizado em: ../outputs/t5_lora_best\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, json\n",
    "\n",
    "BEST_DIR = \"../outputs/t5_lora_best\"\n",
    "metrics_path = \"../outputs/metrics_200_ckpt_6064_vs_9000.json\"\n",
    "m = json.load(open(metrics_path, \"r\"))\n",
    "print(\"Métricas carregadas:\", m)\n",
    "\n",
    "def better(a, b):\n",
    "    if a[\"bleu\"] != b[\"bleu\"]:\n",
    "        return \"6064\" if a[\"bleu\"] > b[\"bleu\"] else \"9000\"\n",
    "    return \"6064\" if a[\"rougeL\"] > b[\"rougeL\"] else \"9000\"\n",
    "\n",
    "choose = None\n",
    "if \"6064\" in m and \"9000\" in m:\n",
    "    choose = better(m[\"6064\"], m[\"9000\"])\n",
    "elif \"6064\" in m:\n",
    "    choose = \"6064\"\n",
    "elif \"9000\" in m:\n",
    "    choose = \"9000\"\n",
    "else:\n",
    "    raise RuntimeError(\"Sem métricas válidas. Rode as células anteriores.\")\n",
    "\n",
    "src = \"../outputs/t5_lora_mps/checkpoint-6064\" if choose==\"6064\" else \"../outputs/t5_lora_mps/checkpoint-9000\"\n",
    "\n",
    "print(f\"Melhor: {choose} → {src}\")\n",
    "if os.path.exists(BEST_DIR):\n",
    "    shutil.rmtree(BEST_DIR)\n",
    "shutil.copytree(src, BEST_DIR)\n",
    "print(\"Modelo final atualizado em:\", BEST_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
