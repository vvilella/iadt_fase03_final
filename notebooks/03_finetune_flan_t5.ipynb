{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft evaluate rouge-score sacrebleu ipywidgets torch\n",
    "\n",
    "import os, torch\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps - macbook\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ee17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "MAX_INPUT_LEN, MAX_TARGET_LEN = 128, 224\n",
    "\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHED = \"cosine\"\n",
    "EPOCHS = 2\n",
    "\n",
    "BATCH = 1           # MPS - macbook\n",
    "GRAD_ACC = 16       # batch efetivo 16\n",
    "SAVE_STEPS = 200    # salvar (se necessário retomra) a cada 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a9af92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca2f255b0654579b5a1fb7cfcc81e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vilella/Documents/fiap/pos_ia/iadt_fase03_final/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf66c4330134821bd2612646e692fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(48500, 1500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\"train\":\"../data/train.jsonl\",\"val\":\"../data/val.jsonl\"})\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(batch[\"input_text\"], max_length=MAX_INPUT_LEN, truncation=True, padding=False)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"target_text\"], max_length=MAX_TARGET_LEN, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=raw[\"train\"].column_names)\n",
    "len(tokenized[\"train\"]), len(tokenized[\"val\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5659da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_cfg = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "                      target_modules=[\"q\",\"v\"])\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f82dda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x483d97250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../outputs/t5_lora_mps\",\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=LR_SCHED,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=50,\n",
    "    fp16=(device==\"cuda\"),\n",
    "    bf16=False,\n",
    "    **({\"eval_strategy\":\"no\"} if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters\n",
    "       else {\"evaluation_strategy\":\"no\"})\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    **({\"processing_class\": tokenizer}\n",
    "       if \"processing_class\" in inspect.signature(Trainer.__init__).parameters\n",
    "       else {\"tokenizer\": tokenizer})\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81f346cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='3032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/3032 : < :, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=0.0, metrics={'train_runtime': 0.0024, 'train_samples_per_second': 40466231.152, 'train_steps_per_second': 1264882.607, 'total_flos': 7612271241062400.0, 'train_loss': 0.0, 'epoch': 2.9689072164948453})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=True)\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f46bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(1k) - ROUGE-L: 0.1255 | BLEU: 1.19\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import evaluate, json\n",
    "\n",
    "# textos da validação (cru, com strings)\n",
    "N = min(1000, len(raw[\"val\"]))\n",
    "texts = raw[\"val\"].select(range(N))[\"input_text\"]\n",
    "refs  = raw[\"val\"].select(range(N))[\"target_text\"]\n",
    "\n",
    "# carregar checkpoint campeão\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model_ckpt = PeftModel.from_pretrained(base, \"../outputs/t5_lora_mps/checkpoint-9000\").to(device)\n",
    "\n",
    "# geração leve\n",
    "preds = []\n",
    "with torch.inference_mode():\n",
    "    for x in texts:\n",
    "        enc = tokenizer(x, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN).to(device)\n",
    "        out = model_ckpt.generate(**enc, max_new_tokens=MAX_TARGET_LEN, num_beams=1)\n",
    "        preds.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        if device==\"mps\": torch.mps.empty_cache()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\"); bleu = evaluate.load(\"sacrebleu\")\n",
    "r = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\n",
    "b = bleu.compute(predictions=preds, references=[[y] for y in refs])\n",
    "print(\"Val(1k) - ROUGE-L:\", round(float(r[\"rougeL\"]),4), \"| BLEU:\", round(float(b[\"score\"]),2))\n",
    "\n",
    "# snapshot\n",
    "import os; os.makedirs(\"../outputs\", exist_ok=True)\n",
    "with open(\"../outputs/eval_snapshot.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"checkpoint\":\"checkpoint-9000\",\"rougeL\":float(r[\"rougeL\"]),\"bleu\":float(b[\"score\"])}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9678ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8e96a44c-1520-44e2-9fd2-d1a9393b51ce)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8e96a44c-1520-44e2-9fd2-d1a9393b51ce)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare salvo em outputs/compare_baseline_vs_finetuned.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, gc\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "rows = [json.loads(l) for l in open(\"../outputs/baseline_val200.jsonl\",\"r\",encoding=\"utf-8\")]\n",
    "inputs = [r[\"input\"] for r in rows]\n",
    "refs200 = [r[\"ref\"] for r in rows]\n",
    "\n",
    "def gen_batch(mdl, texts, bs=2):\n",
    "    out=[]\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0,len(texts),bs):\n",
    "            enc = tokenizer(texts[i:i+bs], return_tensors=\"pt\", padding=True,\n",
    "                            truncation=True, max_length=MAX_INPUT_LEN).to(device)\n",
    "            ids = mdl.generate(**enc, max_new_tokens=MAX_TARGET_LEN, num_beams=1)\n",
    "            out += tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "            if device==\"mps\": torch.mps.empty_cache()\n",
    "    return out\n",
    "\n",
    "# baseline cru\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "preds_bl = gen_batch(base_model, inputs); del base_model; gc.collect()\n",
    "\n",
    "# fine-tuned best\n",
    "preds_ft = gen_batch(model_ckpt, inputs)\n",
    "\n",
    "with open(\"../outputs/compare_baseline_vs_finetuned.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for x, pb, pf, y in zip(inputs, preds_bl, preds_ft, refs200):\n",
    "        f.write(json.dumps({\"input\":x,\"baseline_pred\":pb,\"finetuned_pred\":pf,\"ref\":y},ensure_ascii=False)+\"\\n\")\n",
    "print(\"compare salvo em outputs/compare_baseline_vs_finetuned.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31bc6dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'rougeL': 0.1223, 'bleu': 0.0003}\n",
      "Fine-tuned: {'rougeL': 0.1266, 'bleu': 1.2264}\n"
     ]
    }
   ],
   "source": [
    "import evaluate, json\n",
    "rows = [json.loads(l) for l in open(\"../outputs/compare_baseline_vs_finetuned.jsonl\",\"r\",encoding=\"utf-8\")]\n",
    "preds_bl = [r[\"baseline_pred\"]  for r in rows]\n",
    "preds_ft = [r[\"finetuned_pred\"] for r in rows]\n",
    "refs200  = [r[\"ref\"]            for r in rows]\n",
    "\n",
    "rouge = evaluate.load(\"rouge\"); bleu = evaluate.load(\"sacrebleu\")\n",
    "def metric(preds, refs):\n",
    "    r = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\n",
    "    b = bleu.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    return {\"rougeL\": float(r[\"rougeL\"]), \"bleu\": float(b[\"score\"])}\n",
    "\n",
    "print(\"Baseline:\", {k: round(v,4) for k,v in metric(preds_bl, refs200).items()})\n",
    "print(\"Fine-tuned:\", {k: round(v,4) for k,v in metric(preds_ft, refs200).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a1f945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo final salvo em artifacts/t5_lora_best\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../artifacts/t5_lora_best\", exist_ok=True)\n",
    "model_ckpt.save_pretrained(\"../artifacts/t5_lora_best\")\n",
    "tokenizer.save_pretrained(\"../artifacts/t5_lora_best\")\n",
    "print(\"Modelo final salvo em artifacts/t5_lora_best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
